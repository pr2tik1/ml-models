{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0          2003       196.0       706.0         0.0  ...             0   \n",
       "1          1976         0.0       978.0         0.0  ...             0   \n",
       "2          2002       162.0       486.0         0.0  ...             0   \n",
       "3          1970         0.0       216.0         0.0  ...             0   \n",
       "4          2000       350.0       655.0         0.0  ...             0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0             0            1             0                      0   \n",
       "1             0            1             0                      0   \n",
       "2             0            1             0                      0   \n",
       "3             0            1             0                      0   \n",
       "4             0            1             0                      0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                     0                     0                     1   \n",
       "1                     0                     0                     1   \n",
       "2                     0                     0                     1   \n",
       "3                     0                     0                     0   \n",
       "4                     0                     0                     1   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                      0                  0  \n",
       "1                      0                  0  \n",
       "2                      0                  0  \n",
       "3                      0                  0  \n",
       "4                      0                  0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "data = data_train.append(X_test, ignore_index=True, sort=False)\n",
    "data = pd.get_dummies(data, dummy_na=True, drop_first=True)\n",
    "data.drop('Id', axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fillna(data.median(), inplace=True)\n",
    "columns = data.columns\n",
    "sale_price = data['SalePrice']\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.033420</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.125089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202055</td>\n",
       "      <td>0.038795</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.160959</td>\n",
       "      <td>0.046507</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.10125</td>\n",
       "      <td>0.086109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.133562</td>\n",
       "      <td>0.038561</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.311594</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.038271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.215753</td>\n",
       "      <td>0.060576</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.21875</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.235294     0.150685  0.033420     0.666667        0.500   0.949275   \n",
       "1    0.000000     0.202055  0.038795     0.555556        0.875   0.753623   \n",
       "2    0.235294     0.160959  0.046507     0.666667        0.500   0.934783   \n",
       "3    0.294118     0.133562  0.038561     0.666667        0.500   0.311594   \n",
       "4    0.235294     0.215753  0.060576     0.777778        0.500   0.927536   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_New  \\\n",
       "0      0.883333     0.12250    0.125089         0.0  ...           0.0   \n",
       "1      0.433333     0.00000    0.173281         0.0  ...           0.0   \n",
       "2      0.866667     0.10125    0.086109         0.0  ...           0.0   \n",
       "3      0.333333     0.00000    0.038271         0.0  ...           0.0   \n",
       "4      0.833333     0.21875    0.116052         0.0  ...           0.0   \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_AdjLand  \\\n",
       "0           0.0          1.0           0.0                    0.0   \n",
       "1           0.0          1.0           0.0                    0.0   \n",
       "2           0.0          1.0           0.0                    0.0   \n",
       "3           0.0          1.0           0.0                    0.0   \n",
       "4           0.0          1.0           0.0                    0.0   \n",
       "\n",
       "   SaleCondition_Alloca  SaleCondition_Family  SaleCondition_Normal  \\\n",
       "0                   0.0                   0.0                   1.0   \n",
       "1                   0.0                   0.0                   1.0   \n",
       "2                   0.0                   0.0                   1.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   1.0   \n",
       "\n",
       "   SaleCondition_Partial  SaleCondition_nan  \n",
       "0                    0.0                0.0  \n",
       "1                    0.0                0.0  \n",
       "2                    0.0                0.0  \n",
       "3                    0.0                0.0  \n",
       "4                    0.0                0.0  \n",
       "\n",
       "[5 rows x 289 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), columns = columns)\n",
    "data['SalePrice'] = sale_price\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 289)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[:1460];\n",
    "test = data.iloc[1460:];\n",
    "test.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('SalePrice', axis=1), train['SalePrice'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = np.array_split(X_train, 50)\n",
    "label_batch = np.array_split(y_train, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_batch)):\n",
    "    train_batch[i] = torch.from_numpy(train_batch[i].values).float()\n",
    "for i in range(len(label_batch)):\n",
    "    label_batch[i] = torch.from_numpy(label_batch[i].values).float().view(-1, 1)\n",
    "\n",
    "X_val = torch.from_numpy(X_val.values).float()\n",
    "y_val = torch.from_numpy(y_val.values).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1095, 288)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(288,102),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(102,50),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(50,1),\n",
    "                      nn.ReLU()\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=288, out_features=102, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       "  (3): Linear(in_features=102, out_features=50, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Dropout(p=0.5, inplace=False)\n",
       "  (6): Linear(in_features=50, out_features=1, bias=True)\n",
       "  (7): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = model(train_batch[0])\n",
    "ps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300..  Training Loss: inf..  Test Loss: 15.454.. \n",
      "Epoch: 2/300..  Training Loss: inf..  Test Loss: 15.454.. \n",
      "Epoch: 3/300..  Training Loss: inf..  Test Loss: 15.454.. \n",
      "Epoch: 4/300..  Training Loss: inf..  Test Loss: 11.952.. \n",
      "Epoch: 5/300..  Training Loss: 4.572..  Test Loss: 2.326.. \n",
      "Epoch: 6/300..  Training Loss: 1.804..  Test Loss: 1.205.. \n",
      "Epoch: 7/300..  Training Loss: 1.040..  Test Loss: 0.657.. \n",
      "Epoch: 8/300..  Training Loss: 0.660..  Test Loss: 0.416.. \n",
      "Epoch: 9/300..  Training Loss: 0.541..  Test Loss: 0.369.. \n",
      "Epoch: 10/300..  Training Loss: 0.506..  Test Loss: 0.363.. \n",
      "Epoch: 11/300..  Training Loss: 0.502..  Test Loss: 0.359.. \n",
      "Epoch: 12/300..  Training Loss: 0.474..  Test Loss: 0.351.. \n",
      "Epoch: 13/300..  Training Loss: 0.475..  Test Loss: 0.339.. \n",
      "Epoch: 14/300..  Training Loss: 0.469..  Test Loss: 0.333.. \n",
      "Epoch: 15/300..  Training Loss: 0.454..  Test Loss: 0.327.. \n",
      "Epoch: 16/300..  Training Loss: 0.435..  Test Loss: 0.316.. \n",
      "Epoch: 17/300..  Training Loss: 0.426..  Test Loss: 0.305.. \n",
      "Epoch: 18/300..  Training Loss: 0.425..  Test Loss: 0.298.. \n",
      "Epoch: 19/300..  Training Loss: 0.401..  Test Loss: 0.288.. \n",
      "Epoch: 20/300..  Training Loss: 0.410..  Test Loss: 0.283.. \n",
      "Epoch: 21/300..  Training Loss: 0.403..  Test Loss: 0.274.. \n",
      "Epoch: 22/300..  Training Loss: 0.382..  Test Loss: 0.269.. \n",
      "Epoch: 23/300..  Training Loss: 0.371..  Test Loss: 0.261.. \n",
      "Epoch: 24/300..  Training Loss: 0.369..  Test Loss: 0.254.. \n",
      "Epoch: 25/300..  Training Loss: 0.364..  Test Loss: 0.250.. \n",
      "Epoch: 26/300..  Training Loss: 0.365..  Test Loss: 0.244.. \n",
      "Epoch: 27/300..  Training Loss: 0.363..  Test Loss: 0.242.. \n",
      "Epoch: 28/300..  Training Loss: 0.348..  Test Loss: 0.231.. \n",
      "Epoch: 29/300..  Training Loss: 0.355..  Test Loss: 0.236.. \n",
      "Epoch: 30/300..  Training Loss: 0.352..  Test Loss: 0.229.. \n",
      "Epoch: 31/300..  Training Loss: 0.336..  Test Loss: 0.231.. \n",
      "Epoch: 32/300..  Training Loss: 0.336..  Test Loss: 0.228.. \n",
      "Epoch: 33/300..  Training Loss: 0.345..  Test Loss: 0.219.. \n",
      "Epoch: 34/300..  Training Loss: 0.326..  Test Loss: 0.219.. \n",
      "Epoch: 35/300..  Training Loss: 0.337..  Test Loss: 0.216.. \n",
      "Epoch: 36/300..  Training Loss: 0.313..  Test Loss: 0.215.. \n",
      "Epoch: 37/300..  Training Loss: 0.325..  Test Loss: 0.216.. \n",
      "Epoch: 38/300..  Training Loss: 0.313..  Test Loss: 0.212.. \n",
      "Epoch: 39/300..  Training Loss: 0.319..  Test Loss: 0.211.. \n",
      "Epoch: 40/300..  Training Loss: 0.321..  Test Loss: 0.211.. \n",
      "Epoch: 41/300..  Training Loss: 0.323..  Test Loss: 0.210.. \n",
      "Epoch: 42/300..  Training Loss: 0.318..  Test Loss: 0.208.. \n",
      "Epoch: 43/300..  Training Loss: 0.310..  Test Loss: 0.205.. \n",
      "Epoch: 44/300..  Training Loss: 0.308..  Test Loss: 0.209.. \n",
      "Epoch: 45/300..  Training Loss: 0.311..  Test Loss: 0.205.. \n",
      "Epoch: 46/300..  Training Loss: 0.308..  Test Loss: 0.207.. \n",
      "Epoch: 47/300..  Training Loss: 0.297..  Test Loss: 0.206.. \n",
      "Epoch: 48/300..  Training Loss: 0.303..  Test Loss: 0.199.. \n",
      "Epoch: 49/300..  Training Loss: 0.294..  Test Loss: 0.202.. \n",
      "Epoch: 50/300..  Training Loss: 0.297..  Test Loss: 0.198.. \n",
      "Epoch: 51/300..  Training Loss: 0.294..  Test Loss: 0.201.. \n",
      "Epoch: 52/300..  Training Loss: 0.304..  Test Loss: 0.194.. \n",
      "Epoch: 53/300..  Training Loss: 0.300..  Test Loss: 0.193.. \n",
      "Epoch: 54/300..  Training Loss: 0.304..  Test Loss: 0.192.. \n",
      "Epoch: 55/300..  Training Loss: 0.299..  Test Loss: 0.198.. \n",
      "Epoch: 56/300..  Training Loss: 0.281..  Test Loss: 0.191.. \n",
      "Epoch: 57/300..  Training Loss: 0.296..  Test Loss: 0.193.. \n",
      "Epoch: 58/300..  Training Loss: 0.293..  Test Loss: 0.198.. \n",
      "Epoch: 59/300..  Training Loss: 0.279..  Test Loss: 0.189.. \n",
      "Epoch: 60/300..  Training Loss: 0.299..  Test Loss: 0.192.. \n",
      "Epoch: 61/300..  Training Loss: 0.288..  Test Loss: 0.191.. \n",
      "Epoch: 62/300..  Training Loss: 0.279..  Test Loss: 0.192.. \n",
      "Epoch: 63/300..  Training Loss: 0.279..  Test Loss: 0.190.. \n",
      "Epoch: 64/300..  Training Loss: 0.286..  Test Loss: 0.191.. \n",
      "Epoch: 65/300..  Training Loss: 0.300..  Test Loss: 0.188.. \n",
      "Epoch: 66/300..  Training Loss: 0.282..  Test Loss: 0.187.. \n",
      "Epoch: 67/300..  Training Loss: 0.289..  Test Loss: 0.197.. \n",
      "Epoch: 68/300..  Training Loss: 0.283..  Test Loss: 0.188.. \n",
      "Epoch: 69/300..  Training Loss: 0.285..  Test Loss: 0.193.. \n",
      "Epoch: 70/300..  Training Loss: 0.286..  Test Loss: 0.184.. \n",
      "Epoch: 71/300..  Training Loss: 0.276..  Test Loss: 0.185.. \n",
      "Epoch: 72/300..  Training Loss: 0.265..  Test Loss: 0.184.. \n",
      "Epoch: 73/300..  Training Loss: 0.277..  Test Loss: 0.185.. \n",
      "Epoch: 74/300..  Training Loss: 0.271..  Test Loss: 0.187.. \n",
      "Epoch: 75/300..  Training Loss: 0.275..  Test Loss: 0.189.. \n",
      "Epoch: 76/300..  Training Loss: 0.272..  Test Loss: 0.184.. \n",
      "Epoch: 77/300..  Training Loss: 0.282..  Test Loss: 0.180.. \n",
      "Epoch: 78/300..  Training Loss: 0.275..  Test Loss: 0.180.. \n",
      "Epoch: 79/300..  Training Loss: 0.273..  Test Loss: 0.182.. \n",
      "Epoch: 80/300..  Training Loss: 0.268..  Test Loss: 0.178.. \n",
      "Epoch: 81/300..  Training Loss: 0.279..  Test Loss: 0.180.. \n",
      "Epoch: 82/300..  Training Loss: 0.284..  Test Loss: 0.177.. \n",
      "Epoch: 83/300..  Training Loss: 0.268..  Test Loss: 0.177.. \n",
      "Epoch: 84/300..  Training Loss: 0.272..  Test Loss: 0.176.. \n",
      "Epoch: 85/300..  Training Loss: 0.280..  Test Loss: 0.183.. \n",
      "Epoch: 86/300..  Training Loss: 0.274..  Test Loss: 0.185.. \n",
      "Epoch: 87/300..  Training Loss: 0.270..  Test Loss: 0.177.. \n",
      "Epoch: 88/300..  Training Loss: 0.270..  Test Loss: 0.177.. \n",
      "Epoch: 89/300..  Training Loss: 0.279..  Test Loss: 0.180.. \n",
      "Epoch: 90/300..  Training Loss: 0.266..  Test Loss: 0.178.. \n",
      "Epoch: 91/300..  Training Loss: 0.261..  Test Loss: 0.181.. \n",
      "Epoch: 92/300..  Training Loss: 0.280..  Test Loss: 0.181.. \n",
      "Epoch: 93/300..  Training Loss: 0.271..  Test Loss: 0.180.. \n",
      "Epoch: 94/300..  Training Loss: 0.262..  Test Loss: 0.176.. \n",
      "Epoch: 95/300..  Training Loss: 0.266..  Test Loss: 0.174.. \n",
      "Epoch: 96/300..  Training Loss: 0.264..  Test Loss: 0.177.. \n",
      "Epoch: 97/300..  Training Loss: 0.268..  Test Loss: 0.171.. \n",
      "Epoch: 98/300..  Training Loss: 0.266..  Test Loss: 0.175.. \n",
      "Epoch: 99/300..  Training Loss: 0.257..  Test Loss: 0.184.. \n",
      "Epoch: 100/300..  Training Loss: 0.267..  Test Loss: 0.170.. \n",
      "Epoch: 101/300..  Training Loss: 0.267..  Test Loss: 0.173.. \n",
      "Epoch: 102/300..  Training Loss: 0.272..  Test Loss: 0.180.. \n",
      "Epoch: 103/300..  Training Loss: 0.257..  Test Loss: 0.173.. \n",
      "Epoch: 104/300..  Training Loss: 0.269..  Test Loss: 0.174.. \n",
      "Epoch: 105/300..  Training Loss: 0.262..  Test Loss: 0.168.. \n",
      "Epoch: 106/300..  Training Loss: 0.273..  Test Loss: 0.174.. \n",
      "Epoch: 107/300..  Training Loss: 0.256..  Test Loss: 0.171.. \n",
      "Epoch: 108/300..  Training Loss: 0.263..  Test Loss: 0.173.. \n",
      "Epoch: 109/300..  Training Loss: 0.262..  Test Loss: 0.172.. \n",
      "Epoch: 110/300..  Training Loss: 0.265..  Test Loss: 0.173.. \n",
      "Epoch: 111/300..  Training Loss: 0.258..  Test Loss: 0.168.. \n",
      "Epoch: 112/300..  Training Loss: 0.264..  Test Loss: 0.169.. \n",
      "Epoch: 113/300..  Training Loss: 0.250..  Test Loss: 0.179.. \n",
      "Epoch: 114/300..  Training Loss: 0.272..  Test Loss: 0.168.. \n",
      "Epoch: 115/300..  Training Loss: 0.260..  Test Loss: 0.167.. \n",
      "Epoch: 116/300..  Training Loss: 0.262..  Test Loss: 0.178.. \n",
      "Epoch: 117/300..  Training Loss: 0.255..  Test Loss: 0.172.. \n",
      "Epoch: 118/300..  Training Loss: 0.253..  Test Loss: 0.169.. \n",
      "Epoch: 119/300..  Training Loss: 0.259..  Test Loss: 0.170.. \n",
      "Epoch: 120/300..  Training Loss: 0.270..  Test Loss: 0.168.. \n",
      "Epoch: 121/300..  Training Loss: 0.253..  Test Loss: 0.173.. \n",
      "Epoch: 122/300..  Training Loss: 0.267..  Test Loss: 0.170.. \n",
      "Epoch: 123/300..  Training Loss: 0.266..  Test Loss: 0.179.. \n",
      "Epoch: 124/300..  Training Loss: 0.257..  Test Loss: 0.188.. \n",
      "Epoch: 125/300..  Training Loss: 0.252..  Test Loss: 0.169.. \n",
      "Epoch: 126/300..  Training Loss: 0.265..  Test Loss: 0.165.. \n",
      "Epoch: 127/300..  Training Loss: 0.262..  Test Loss: 0.164.. \n",
      "Epoch: 128/300..  Training Loss: 0.257..  Test Loss: 0.168.. \n",
      "Epoch: 129/300..  Training Loss: 0.262..  Test Loss: 0.164.. \n",
      "Epoch: 130/300..  Training Loss: 0.258..  Test Loss: 0.166.. \n",
      "Epoch: 131/300..  Training Loss: 0.264..  Test Loss: 0.170.. \n",
      "Epoch: 132/300..  Training Loss: 0.266..  Test Loss: 0.162.. \n",
      "Epoch: 133/300..  Training Loss: 0.250..  Test Loss: 0.168.. \n",
      "Epoch: 134/300..  Training Loss: 0.251..  Test Loss: 0.167.. \n",
      "Epoch: 135/300..  Training Loss: 0.257..  Test Loss: 0.163.. \n",
      "Epoch: 136/300..  Training Loss: 0.261..  Test Loss: 0.163.. \n",
      "Epoch: 137/300..  Training Loss: 0.257..  Test Loss: 0.171.. \n",
      "Epoch: 138/300..  Training Loss: 0.244..  Test Loss: 0.166.. \n",
      "Epoch: 139/300..  Training Loss: 0.257..  Test Loss: 0.170.. \n",
      "Epoch: 140/300..  Training Loss: 0.255..  Test Loss: 0.169.. \n",
      "Epoch: 141/300..  Training Loss: 0.252..  Test Loss: 0.169.. \n",
      "Epoch: 142/300..  Training Loss: 0.247..  Test Loss: 0.162.. \n",
      "Epoch: 143/300..  Training Loss: 0.265..  Test Loss: 0.169.. \n",
      "Epoch: 144/300..  Training Loss: 0.251..  Test Loss: 0.164.. \n",
      "Epoch: 145/300..  Training Loss: 0.258..  Test Loss: 0.164.. \n",
      "Epoch: 146/300..  Training Loss: 0.254..  Test Loss: 0.160.. \n",
      "Epoch: 147/300..  Training Loss: 0.258..  Test Loss: 0.162.. \n",
      "Epoch: 148/300..  Training Loss: 0.259..  Test Loss: 0.159.. \n",
      "Epoch: 149/300..  Training Loss: 0.254..  Test Loss: 0.160.. \n",
      "Epoch: 150/300..  Training Loss: 0.251..  Test Loss: 0.165.. \n",
      "Epoch: 151/300..  Training Loss: 0.257..  Test Loss: 0.166.. \n",
      "Epoch: 152/300..  Training Loss: 0.252..  Test Loss: 0.158.. \n",
      "Epoch: 153/300..  Training Loss: 0.257..  Test Loss: 0.161.. \n",
      "Epoch: 154/300..  Training Loss: 0.253..  Test Loss: 0.161.. \n",
      "Epoch: 155/300..  Training Loss: 0.254..  Test Loss: 0.162.. \n",
      "Epoch: 156/300..  Training Loss: 0.256..  Test Loss: 0.161.. \n",
      "Epoch: 157/300..  Training Loss: 0.257..  Test Loss: 0.159.. \n",
      "Epoch: 158/300..  Training Loss: 0.250..  Test Loss: 0.173.. \n",
      "Epoch: 159/300..  Training Loss: 0.259..  Test Loss: 0.165.. \n",
      "Epoch: 160/300..  Training Loss: 0.253..  Test Loss: 0.163.. \n",
      "Epoch: 161/300..  Training Loss: 0.257..  Test Loss: 0.159.. \n",
      "Epoch: 162/300..  Training Loss: 0.250..  Test Loss: 0.156.. \n",
      "Epoch: 163/300..  Training Loss: 0.256..  Test Loss: 0.160.. \n",
      "Epoch: 164/300..  Training Loss: 0.263..  Test Loss: 0.168.. \n",
      "Epoch: 165/300..  Training Loss: 0.252..  Test Loss: 0.161.. \n",
      "Epoch: 166/300..  Training Loss: 0.254..  Test Loss: 0.158.. \n",
      "Epoch: 167/300..  Training Loss: 0.252..  Test Loss: 0.163.. \n",
      "Epoch: 168/300..  Training Loss: 0.251..  Test Loss: 0.159.. \n",
      "Epoch: 169/300..  Training Loss: 0.257..  Test Loss: 0.166.. \n",
      "Epoch: 170/300..  Training Loss: 0.250..  Test Loss: 0.159.. \n",
      "Epoch: 171/300..  Training Loss: 0.250..  Test Loss: 0.164.. \n",
      "Epoch: 172/300..  Training Loss: 0.257..  Test Loss: 0.157.. \n",
      "Epoch: 173/300..  Training Loss: 0.252..  Test Loss: 0.162.. \n",
      "Epoch: 174/300..  Training Loss: 0.256..  Test Loss: 0.162.. \n",
      "Epoch: 175/300..  Training Loss: 0.253..  Test Loss: 0.163.. \n",
      "Epoch: 176/300..  Training Loss: 0.253..  Test Loss: 0.161.. \n",
      "Epoch: 177/300..  Training Loss: 0.258..  Test Loss: 0.154.. \n",
      "Epoch: 178/300..  Training Loss: 0.263..  Test Loss: 0.164.. \n",
      "Epoch: 179/300..  Training Loss: 0.244..  Test Loss: 0.155.. \n",
      "Epoch: 180/300..  Training Loss: 0.252..  Test Loss: 0.172.. \n",
      "Epoch: 181/300..  Training Loss: 0.261..  Test Loss: 0.156.. \n",
      "Epoch: 182/300..  Training Loss: 0.256..  Test Loss: 0.156.. \n",
      "Epoch: 183/300..  Training Loss: 0.256..  Test Loss: 0.153.. \n",
      "Epoch: 184/300..  Training Loss: 0.247..  Test Loss: 0.156.. \n",
      "Epoch: 185/300..  Training Loss: 0.246..  Test Loss: 0.156.. \n",
      "Epoch: 186/300..  Training Loss: 0.258..  Test Loss: 0.161.. \n",
      "Epoch: 187/300..  Training Loss: 0.255..  Test Loss: 0.152.. \n",
      "Epoch: 188/300..  Training Loss: 0.245..  Test Loss: 0.153.. \n",
      "Epoch: 189/300..  Training Loss: 0.253..  Test Loss: 0.153.. \n",
      "Epoch: 190/300..  Training Loss: 0.265..  Test Loss: 0.153.. \n",
      "Epoch: 191/300..  Training Loss: 0.263..  Test Loss: 0.155.. \n",
      "Epoch: 192/300..  Training Loss: 0.250..  Test Loss: 0.154.. \n",
      "Epoch: 193/300..  Training Loss: 0.255..  Test Loss: 0.156.. \n",
      "Epoch: 194/300..  Training Loss: 0.259..  Test Loss: 0.157.. \n",
      "Epoch: 195/300..  Training Loss: 0.262..  Test Loss: 0.152.. \n",
      "Epoch: 196/300..  Training Loss: 0.246..  Test Loss: 0.151.. \n",
      "Epoch: 197/300..  Training Loss: 0.252..  Test Loss: 0.153.. \n",
      "Epoch: 198/300..  Training Loss: 0.260..  Test Loss: 0.168.. \n",
      "Epoch: 199/300..  Training Loss: 0.249..  Test Loss: 0.151.. \n",
      "Epoch: 200/300..  Training Loss: 0.248..  Test Loss: 0.164.. \n",
      "Epoch: 201/300..  Training Loss: 0.252..  Test Loss: 0.151.. \n",
      "Epoch: 202/300..  Training Loss: 0.256..  Test Loss: 0.150.. \n",
      "Epoch: 203/300..  Training Loss: 0.253..  Test Loss: 0.155.. \n",
      "Epoch: 204/300..  Training Loss: 0.262..  Test Loss: 0.150.. \n",
      "Epoch: 205/300..  Training Loss: 0.268..  Test Loss: 0.149.. \n",
      "Epoch: 206/300..  Training Loss: 0.253..  Test Loss: 0.152.. \n",
      "Epoch: 207/300..  Training Loss: 0.247..  Test Loss: 0.155.. \n",
      "Epoch: 208/300..  Training Loss: 0.255..  Test Loss: 0.149.. \n",
      "Epoch: 209/300..  Training Loss: 0.252..  Test Loss: 0.152.. \n",
      "Epoch: 210/300..  Training Loss: 0.257..  Test Loss: 0.153.. \n",
      "Epoch: 211/300..  Training Loss: 0.252..  Test Loss: 0.156.. \n",
      "Epoch: 212/300..  Training Loss: 0.259..  Test Loss: 0.156.. \n",
      "Epoch: 213/300..  Training Loss: 0.262..  Test Loss: 0.157.. \n",
      "Epoch: 214/300..  Training Loss: 0.249..  Test Loss: 0.155.. \n",
      "Epoch: 215/300..  Training Loss: 0.241..  Test Loss: 0.152.. \n",
      "Epoch: 216/300..  Training Loss: 0.250..  Test Loss: 0.152.. \n",
      "Epoch: 217/300..  Training Loss: 0.256..  Test Loss: 0.150.. \n",
      "Epoch: 218/300..  Training Loss: 0.246..  Test Loss: 0.155.. \n",
      "Epoch: 219/300..  Training Loss: 0.242..  Test Loss: 0.152.. \n",
      "Epoch: 220/300..  Training Loss: 0.238..  Test Loss: 0.150.. \n",
      "Epoch: 221/300..  Training Loss: 0.252..  Test Loss: 0.153.. \n",
      "Epoch: 222/300..  Training Loss: 0.252..  Test Loss: 0.153.. \n",
      "Epoch: 223/300..  Training Loss: 0.250..  Test Loss: 0.155.. \n",
      "Epoch: 224/300..  Training Loss: 0.247..  Test Loss: 0.151.. \n",
      "Epoch: 225/300..  Training Loss: 0.248..  Test Loss: 0.148.. \n",
      "Epoch: 226/300..  Training Loss: 0.248..  Test Loss: 0.148.. \n",
      "Epoch: 227/300..  Training Loss: 0.253..  Test Loss: 0.148.. \n",
      "Epoch: 228/300..  Training Loss: 0.255..  Test Loss: 0.148.. \n",
      "Epoch: 229/300..  Training Loss: 0.250..  Test Loss: 0.147.. \n",
      "Epoch: 230/300..  Training Loss: 0.244..  Test Loss: 0.150.. \n",
      "Epoch: 231/300..  Training Loss: 0.236..  Test Loss: 0.152.. \n",
      "Epoch: 232/300..  Training Loss: 0.252..  Test Loss: 0.150.. \n",
      "Epoch: 233/300..  Training Loss: 0.253..  Test Loss: 0.154.. \n",
      "Epoch: 234/300..  Training Loss: 0.246..  Test Loss: 0.149.. \n",
      "Epoch: 235/300..  Training Loss: 0.248..  Test Loss: 0.151.. \n",
      "Epoch: 236/300..  Training Loss: 0.247..  Test Loss: 0.149.. \n",
      "Epoch: 237/300..  Training Loss: 0.249..  Test Loss: 0.148.. \n",
      "Epoch: 238/300..  Training Loss: 0.250..  Test Loss: 0.149.. \n",
      "Epoch: 239/300..  Training Loss: 0.254..  Test Loss: 0.150.. \n",
      "Epoch: 240/300..  Training Loss: 0.244..  Test Loss: 0.149.. \n",
      "Epoch: 241/300..  Training Loss: 0.252..  Test Loss: 0.149.. \n",
      "Epoch: 242/300..  Training Loss: 0.249..  Test Loss: 0.149.. \n",
      "Epoch: 243/300..  Training Loss: 0.247..  Test Loss: 0.147.. \n",
      "Epoch: 244/300..  Training Loss: 0.250..  Test Loss: 0.146.. \n",
      "Epoch: 245/300..  Training Loss: 0.242..  Test Loss: 0.147.. \n",
      "Epoch: 246/300..  Training Loss: 0.243..  Test Loss: 0.158.. \n",
      "Epoch: 247/300..  Training Loss: 0.247..  Test Loss: 0.149.. \n",
      "Epoch: 248/300..  Training Loss: 0.255..  Test Loss: 0.148.. \n",
      "Epoch: 249/300..  Training Loss: 0.249..  Test Loss: 0.147.. \n",
      "Epoch: 250/300..  Training Loss: 0.254..  Test Loss: 0.148.. \n",
      "Epoch: 251/300..  Training Loss: 0.255..  Test Loss: 0.148.. \n",
      "Epoch: 252/300..  Training Loss: 0.254..  Test Loss: 0.149.. \n",
      "Epoch: 253/300..  Training Loss: 0.250..  Test Loss: 0.151.. \n",
      "Epoch: 254/300..  Training Loss: 0.252..  Test Loss: 0.149.. \n",
      "Epoch: 255/300..  Training Loss: 0.252..  Test Loss: 0.152.. \n",
      "Epoch: 256/300..  Training Loss: 0.257..  Test Loss: 0.152.. \n",
      "Epoch: 257/300..  Training Loss: 0.252..  Test Loss: 0.149.. \n",
      "Epoch: 258/300..  Training Loss: 0.257..  Test Loss: 0.155.. \n",
      "Epoch: 259/300..  Training Loss: 0.242..  Test Loss: 0.149.. \n",
      "Epoch: 260/300..  Training Loss: 0.249..  Test Loss: 0.151.. \n",
      "Epoch: 261/300..  Training Loss: 0.243..  Test Loss: 0.146.. \n",
      "Epoch: 262/300..  Training Loss: 0.246..  Test Loss: 0.146.. \n",
      "Epoch: 263/300..  Training Loss: 0.245..  Test Loss: 0.150.. \n",
      "Epoch: 264/300..  Training Loss: 0.258..  Test Loss: 0.147.. \n",
      "Epoch: 265/300..  Training Loss: 0.251..  Test Loss: 0.165.. \n",
      "Epoch: 266/300..  Training Loss: 0.246..  Test Loss: 0.158.. \n",
      "Epoch: 267/300..  Training Loss: 0.246..  Test Loss: 0.147.. \n",
      "Epoch: 268/300..  Training Loss: 0.252..  Test Loss: 0.151.. \n",
      "Epoch: 269/300..  Training Loss: 0.246..  Test Loss: 0.149.. \n",
      "Epoch: 270/300..  Training Loss: 0.250..  Test Loss: 0.145.. \n",
      "Epoch: 271/300..  Training Loss: 0.261..  Test Loss: 0.148.. \n",
      "Epoch: 272/300..  Training Loss: 0.256..  Test Loss: 0.148.. \n",
      "Epoch: 273/300..  Training Loss: 0.257..  Test Loss: 0.148.. \n",
      "Epoch: 274/300..  Training Loss: 0.255..  Test Loss: 0.151.. \n",
      "Epoch: 275/300..  Training Loss: 0.239..  Test Loss: 0.148.. \n",
      "Epoch: 276/300..  Training Loss: 0.254..  Test Loss: 0.159.. \n",
      "Epoch: 277/300..  Training Loss: 0.252..  Test Loss: 0.145.. \n",
      "Epoch: 278/300..  Training Loss: 0.251..  Test Loss: 0.145.. \n",
      "Epoch: 279/300..  Training Loss: 0.255..  Test Loss: 0.151.. \n",
      "Epoch: 280/300..  Training Loss: 0.248..  Test Loss: 0.145.. \n",
      "Epoch: 281/300..  Training Loss: 0.240..  Test Loss: 0.151.. \n",
      "Epoch: 282/300..  Training Loss: 0.242..  Test Loss: 0.146.. \n",
      "Epoch: 283/300..  Training Loss: 0.254..  Test Loss: 0.148.. \n",
      "Epoch: 284/300..  Training Loss: 0.247..  Test Loss: 0.154.. \n",
      "Epoch: 285/300..  Training Loss: 0.246..  Test Loss: 0.150.. \n",
      "Epoch: 286/300..  Training Loss: 0.259..  Test Loss: 0.146.. \n",
      "Epoch: 287/300..  Training Loss: 0.249..  Test Loss: 0.153.. \n",
      "Epoch: 288/300..  Training Loss: 0.248..  Test Loss: 0.151.. \n",
      "Epoch: 289/300..  Training Loss: 0.247..  Test Loss: 0.150.. \n",
      "Epoch: 290/300..  Training Loss: 0.256..  Test Loss: 0.151.. \n",
      "Epoch: 291/300..  Training Loss: 0.247..  Test Loss: 0.156.. \n",
      "Epoch: 292/300..  Training Loss: 0.253..  Test Loss: 0.147.. \n",
      "Epoch: 293/300..  Training Loss: 0.245..  Test Loss: 0.151.. \n",
      "Epoch: 294/300..  Training Loss: 0.254..  Test Loss: 0.145.. \n",
      "Epoch: 295/300..  Training Loss: 0.251..  Test Loss: 0.148.. \n",
      "Epoch: 296/300..  Training Loss: 0.242..  Test Loss: 0.150.. \n",
      "Epoch: 297/300..  Training Loss: 0.253..  Test Loss: 0.149.. \n",
      "Epoch: 298/300..  Training Loss: 0.250..  Test Loss: 0.148.. \n",
      "Epoch: 299/300..  Training Loss: 0.239..  Test Loss: 0.149.. \n",
      "Epoch: 300/300..  Training Loss: 0.234..  Test Loss: 0.146.. \n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i in range(len(train_batch)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_batch[i])\n",
    "        loss = torch.sqrt(criterion(torch.log(output), torch.log(label_batch[i])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            predictions = model(X_val)\n",
    "            test_loss += torch.sqrt(criterion(torch.log(predictions), torch.log(y_val)))\n",
    "                \n",
    "        train_losses.append(train_loss/len(train_batch))\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(train_loss/len(train_batch)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f48d0567c50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1459, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.from_numpy(test.values).float()\n",
    "model.eval()\n",
    "output = model.forward(test)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['SalePrice'] = output.detach().numpy()\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>114515.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>153945.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>176706.671875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>190572.484375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>202145.171875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  114515.843750\n",
       "1  1462  153945.265625\n",
       "2  1463  176706.671875\n",
       "3  1464  190572.484375\n",
       "4  1465  202145.171875"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
